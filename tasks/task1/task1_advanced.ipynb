{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kateksgva/fff/blob/main/tasks/task1/task1_advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 1: разработка пайплайна предобработки текста с ООП\n",
        "\n",
        "#### Цель:\n",
        "Hазработать класс на Python с использованием принципов объектно-ориентированного программирования (ООП), который реализует пайплайн для предобработки текста\n",
        "\n",
        "#### Методы, которые должен реализовывать разработанный класс:\n",
        "1. Токенизация\n",
        "2. Лемматизация\n",
        "3. Удаление стоп-слов\n",
        "\n",
        "Инструкция содержит подробное описание процесса создания класса. Результат вашей работы разместите в одной ячейке ниже инструкции"
      ],
      "metadata": {
        "id": "LHe6Z-d9h1Ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Инструкция\n",
        "\n",
        "**Создание класса**\n",
        "\n",
        "Определите класс `TextProcessor`, который будет содержать методы для предобработки текста.  \n",
        "\n",
        "```python\n",
        "# Создание базового класса для предобработки текста\n",
        "class TextProcessor:\n",
        "    def __init__(self, text):\n",
        "        \"\"\"\n",
        "        Инициализация класса с исходным текстом.\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "        self.tokens = []\n",
        "        self.cleaned_tokens = []\n",
        "```\n",
        "\n",
        "**Метод токенизации**\n",
        "\n",
        "Реализуйте метод, который разделяет текст на отдельные слова.\n",
        "\n",
        "```python\n",
        "    def tokenize(self):\n",
        "        \"\"\"\n",
        "        Метод для токенизации текста.\n",
        "        \"\"\"\n",
        "        # Реализуйте токенизацию здесь\n",
        "        pass\n",
        "```\n",
        "\n",
        "Пример вызова метода:  \n",
        "```python\n",
        "processor = TextProcessor(text)\n",
        "processor.tokenize()\n",
        "print(processor.tokens)\n",
        "```\n",
        "\n",
        "**Метод лемматизации**\n",
        "\n",
        "Добавьте метод, который преобразует слова к их леммам. Используйте `WordNetLemmatizer` из NLTK.\n",
        "\n",
        "```python\n",
        "    def lemmatize(self):\n",
        "        \"\"\"\n",
        "        Метод для лемматизации токенов.\n",
        "        \"\"\"\n",
        "        # Реализуйте лемматизацию здесь\n",
        "        pass\n",
        "```\n",
        "\n",
        "Пример вызова метода:  \n",
        "```python\n",
        "processor.lemmatize()\n",
        "print(processor.cleaned_tokens)\n",
        "```\n",
        "\n",
        "**Метод удаления стоп-слов**\n",
        "\n",
        "Добавьте метод для удаления стоп-слов из токенов. Используйте список стоп-слов из NLTK.\n",
        "\n",
        "```python\n",
        "    def remove_stopwords(self):\n",
        "        \"\"\"\n",
        "        Метод для удаления стоп-слов.\n",
        "        \"\"\"\n",
        "        # Реализуйте удаление стоп-слов здесь\n",
        "        pass\n",
        "```\n",
        "\n",
        "Пример вызова метода:  \n",
        "```python\n",
        "processor.remove_stopwords()\n",
        "print(processor.cleaned_tokens)\n",
        "```\n",
        "\n",
        "**Запуск пайплайна**\n",
        "\n",
        "Объедините все шаги в пайплайн. Добавьте вызов каждого метода по порядку:\n",
        "\n",
        "```python\n",
        "processor = TextProcessor(text)\n",
        "processor.tokenize()\n",
        "processor.remove_stopwords()\n",
        "processor.lemmatize()\n",
        "\n",
        "# Вывод итогового результата\n",
        "print(\"Токены:\", processor.tokens)\n",
        "print(\"Лемматизированные токены:\", processor.cleaned_tokens)\n",
        "```"
      ],
      "metadata": {
        "id": "Z4PuMmgPimOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### ВАШЕ РЕШЕНИЕ ЗДЕСЬ\n",
        "# Установка нужных библиотек из nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Загружаем необходимые ресурсы\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Загружаем файл с текстом\n",
        "!wget    https://raw.githubusercontent.com/kateksgva/kate.ksgva.hse/refs/heads/main/text%20hw1\n",
        "with open(\"text hw1\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "#  Создание базового класса для работы с текстом, задаем пустые списки\n",
        "class TextProcessor:\n",
        "    def __init__(self, text):\n",
        "    self.text = text\n",
        "    self.tokens = []\n",
        "    self.cleaned_tokens = []\n",
        "    self.lemmatized_tokens = []\n",
        "\n",
        "# Токенизируем текст\n",
        "def tokenize(self):\n",
        "  self.tokens = word_tokenize(self.text)\n",
        "        return self.tokens\n",
        "# Лемматизируем\n",
        "def lemmatize(self):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "        self.lemmatized_tokens = [lemmatizer.lemmatize(token) for token in self.cleaned_tokens]\n",
        "        return self.lemmatized_tokens\n",
        "# Удаление стоп-слов\n",
        "def remove_stopwords(self):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "        self.cleaned_tokens = [token for token in self.tokens if token not in stop_words]\n",
        "        self.cleaned_tokens\n",
        "\n",
        "processor = TextProcessor(text)\n",
        "processor.tokenize()\n",
        "processor.remove_stopwords()\n",
        "processor.lemmatize()\n",
        "print(processor.cleaned_tokens)\n",
        "\n",
        "\n",
        "# Выводим результаты\n",
        "print(\"Токены:\", processor.tokens)\n",
        "print(\"Очищенные токены:\", processor.cleaned_tokens) # Выводим cleaned_tokens, а не tokens после remove_stopwords\n",
        "print(\"Лемматизированные токены:\", processor.lemmatized_tokens)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UHqMdoUip-We",
        "outputId": "fe75bee2-f7be-4ac9-85ce-62997aad0917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 19 (<ipython-input-17-ddb943df38cd>, line 20)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-ddb943df38cd>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    self.text = text\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Критерии оценивания**\n",
        "\n",
        "- **Отсутствие ошибок в ООП:** класс корректно инициализируется и выполняет все методы без ошибок (2 балла).  \n",
        "- **Реализован метод токенизации:** текст корректно разделяется на токены (2 балла).  \n",
        "- **Реализован метод лемматизации:** все токены преобразованы к леммам (2 балла).  \n",
        "- **Реализован метод удаления стоп-слов:** стоп-слова корректно удалены из токенов (2 балла).  \n",
        "- **Класс протестирован:** все методы вызваны, код работает (2 балла).  \n",
        "\n",
        "Общий балл: **10 баллов**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Примечания**\n",
        "\n",
        "- Добавьте текстовые ячейки в Colab и комментарии с описанием этапов выполнения.\n",
        "- Комментарии не оцениваются, но они важны для вашей работы и воспроизводимости кода\n",
        "- Проверьте, что все методы выполняются корректно на примере любого текста.\n",
        "- Пример текста для проверки работы пайплайна: `https://github.com/vifirsanova/compling/blob/main/tasks/task1/data.txt`."
      ],
      "metadata": {
        "id": "Kk81rQiAqBjr"
      }
    }
  ]
}